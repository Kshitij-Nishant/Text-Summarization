{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\1. Python Important\\\\Self Practice\\\\ML Practice\\\\NLP\\\\Text-Summarization\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\1. Python Important\\\\Self Practice\\\\ML Practice\\\\NLP\\\\Text-Summarization'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelTrainerConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path\n",
    "    model_ckpt: Path\n",
    "    num_train_epochs: int\n",
    "    warmup_steps: int\n",
    "    per_device_train_batch_size: int\n",
    "    weight_decay: float\n",
    "    logging_steps: int\n",
    "    evaluation_strategy: str\n",
    "    eval_steps: int\n",
    "    save_steps: float\n",
    "    gradient_accumulation_steps: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textsumarrizer.constants import *\n",
    "from textsumarrizer.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    \n",
    "    def get_model_trainer_config(self) -> ModelTrainerConfig:\n",
    "        config = self.config.model_trainer\n",
    "        params = self.params.TrainingArguments\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        model_trainer_config = ModelTrainerConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            data_path=config.data_path,\n",
    "            model_ckpt = config.model_ckpt,\n",
    "            num_train_epochs = params.num_train_epochs,\n",
    "            warmup_steps = params.warmup_steps,\n",
    "            per_device_train_batch_size = params.per_device_train_batch_size,\n",
    "            weight_decay = params.weight_decay,\n",
    "            logging_steps = params.logging_steps,\n",
    "            evaluation_strategy = params.evaluation_strategy,\n",
    "            eval_steps = params.evaluation_strategy,\n",
    "            save_steps = params.save_steps,\n",
    "            gradient_accumulation_steps = params.gradient_accumulation_steps\n",
    "        )\n",
    "\n",
    "        return model_trainer_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\randy\\anaconda3\\envs\\textS\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-06-26 00:08:23,198: INFO: config: PyTorch version 2.3.1+cu121 available.]\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self, config: ModelTrainerConfig):\n",
    "        self.config = config\n",
    "\n",
    "\n",
    "    \n",
    "    def train(self):\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.config.model_ckpt)\n",
    "        model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(self.config.model_ckpt).to(device)\n",
    "        seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model_pegasus)\n",
    "        \n",
    "        #loading data \n",
    "        dataset_samsum_pt = load_from_disk(self.config.data_path)\n",
    "\n",
    "        \"\"\"\n",
    "        # Execute this if params need to be set via params.yaml file\n",
    "        trainer_args = TrainingArguments(\n",
    "             output_dir=self.config.root_dir, num_train_epochs=self.config.num_train_epochs, warmup_steps=self.config.warmup_steps,\n",
    "             per_device_train_batch_size=self.config.per_device_train_batch_size, per_device_eval_batch_size=self.config.per_device_train_batch_size,\n",
    "             weight_decay=self.config.weight_decay, logging_steps=self.config.logging_steps,\n",
    "             evaluation_strategy=self.config.evaluation_strategy, eval_steps=self.config.eval_steps, save_steps=1e6,\n",
    "             gradient_accumulation_steps=self.config.gradient_accumulation_steps\n",
    "         )\n",
    "        \"\"\"\n",
    "\n",
    "        trainer_args = TrainingArguments(\n",
    "            output_dir=self.config.root_dir, num_train_epochs=0.01, warmup_steps=10,\n",
    "            per_device_train_batch_size=1, per_device_eval_batch_size=1,\n",
    "            weight_decay=0.01, logging_steps=10,\n",
    "            evaluation_strategy='steps', eval_steps=10, save_steps=1e6,\n",
    "            gradient_accumulation_steps=2,  # Adjust according to your memory constraints\n",
    "            fp16=True,  # Enable mixed precision training\n",
    "            gradient_checkpointing=True,  # Enable gradient checkpointing\n",
    "            report_to=\"none\"  # Disable reporting to minimize memory overhead\n",
    "        ) \n",
    "\n",
    "        trainer = Trainer(model=model_pegasus, args=trainer_args,\n",
    "                  tokenizer=tokenizer, data_collator=seq2seq_data_collator,\n",
    "                  train_dataset=dataset_samsum_pt[\"train\"], \n",
    "                  # dataset_samsum_pt[\"test\"] : take test data to train as it has less datapoints and hence will be less computationally expensive\n",
    "                  # ideally should us dataset_samsum_pt[\"train\"]\n",
    "                  eval_dataset=dataset_samsum_pt[\"validation\"])\n",
    "        \n",
    "        trainer.train()\n",
    "\n",
    "        ## Save model\n",
    "        model_pegasus.save_pretrained(os.path.join(self.config.root_dir,\"pegasus-samsum-model\"))\n",
    "        ## Save tokenizer\n",
    "        tokenizer.save_pretrained(os.path.join(self.config.root_dir,\"tokenizer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "# Clearing the CUDA cache before training can sometimes free up enough memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WARNING: \"You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-06-26 00:08:23,769: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-06-26 00:08:23,774: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-06-26 00:08:23,775: INFO: common: created directory at: artifacts]\n",
      "[2024-06-26 00:08:23,776: INFO: common: created directory at: artifacts/model_trainer]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\randy\\anaconda3\\envs\\textS\\lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "  0%|          | 0/74 [00:00<?, ?it/s]c:\\Users\\randy\\anaconda3\\envs\\textS\\lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      " 14%|â–ˆâ–Ž        | 10/74 [00:54<05:15,  4.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.1975, 'grad_norm': 23.971405029296875, 'learning_rate': 4e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 14%|â–ˆâ–Ž        | 10/74 [04:50<05:15,  4.92s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.4174280166625977, 'eval_runtime': 235.965, 'eval_samples_per_second': 3.467, 'eval_steps_per_second': 3.467, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|â–ˆâ–ˆâ–‹       | 20/74 [05:38<07:00,  7.79s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7978, 'grad_norm': 19.381608963012695, 'learning_rate': 4.453125e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 27%|â–ˆâ–ˆâ–‹       | 20/74 [09:45<07:00,  7.79s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.105271816253662, 'eval_runtime': 247.1257, 'eval_samples_per_second': 3.31, 'eval_steps_per_second': 3.31, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 30/74 [11:13<08:38, 11.78s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.788, 'grad_norm': 154.64639282226562, 'learning_rate': 3.671875e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 30/74 [16:21<08:38, 11.78s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9806020259857178, 'eval_runtime': 308.6695, 'eval_samples_per_second': 2.65, 'eval_steps_per_second': 2.65, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 40/74 [17:34<06:21, 11.22s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.914, 'grad_norm': 12.137225151062012, 'learning_rate': 2.890625e-05, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 40/74 [22:15<06:21, 11.22s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9035159349441528, 'eval_runtime': 281.0449, 'eval_samples_per_second': 2.911, 'eval_steps_per_second': 2.911, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 50/74 [23:24<04:16, 10.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4647, 'grad_norm': 20.3756103515625, 'learning_rate': 2.1875e-05, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 50/74 [29:15<04:16, 10.69s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8697023391723633, 'eval_runtime': 351.2388, 'eval_samples_per_second': 2.329, 'eval_steps_per_second': 2.329, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 60/74 [30:57<03:19, 14.25s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.805, 'grad_norm': 11.69636058807373, 'learning_rate': 1.4062500000000001e-05, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 60/74 [37:16<03:19, 14.25s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8455625772476196, 'eval_runtime': 378.9315, 'eval_samples_per_second': 2.159, 'eval_steps_per_second': 2.159, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 70/74 [38:39<00:52, 13.24s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4953, 'grad_norm': 41.65949249267578, 'learning_rate': 6.25e-06, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 70/74 [45:05<00:52, 13.24s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8350369930267334, 'eval_runtime': 385.2346, 'eval_samples_per_second': 2.123, 'eval_steps_per_second': 2.123, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 74/74 [45:43<00:00, 37.07s/it] \n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 128, 'min_length': 32, 'num_beams': 8, 'length_penalty': 0.8, 'forced_eos_token_id': 1}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 2743.1187, 'train_samples_per_second': 0.054, 'train_steps_per_second': 0.027, 'train_loss': 2.6631710722639754, 'epoch': 0.01}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    model_trainer_config = config.get_model_trainer_config()\n",
    "    model_trainer_config = ModelTrainer(config=model_trainer_config)\n",
    "    model_trainer_config.train()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.__version__\n",
    "# to check if torch version installed is CPU or CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to download latest CUDA version for Pytorch\n",
    "# pip install torch==2.3.1+cu121 torchvision==0.18.1+cu121 torchaudio==2.3.1+cu121 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A module that was compiled using NumPy 1.x cannot be run in\n",
    "NumPy 2.0.0 as it may crash. To support both 1.x and 2.x\n",
    "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
    "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
    "\n",
    "If you are a user of the module, the easiest solution will be to\n",
    "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
    "We expect that some modules will need time to support NumPy 2.\n",
    "\n",
    "\n",
    "Due to compatibility issues between Numpy and Pytorch, I have downgraded Numpy-2.0.0 to 1.26.4 for this textS venv.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
